integrations:
- integration_type: git_repo
  git_repo: sashaDoubov/llm-foundry
  git_branch: sasha/pile-of-law # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[llm]
  ssh_clone: false # Should be true if using a private repo
- integration_type: wandb
  project: sasha-onboarding
  entity: mosaic-ml

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `examples/llm/README.md`
# to convert and host the full 'train' dataset.
command: |
  cd llm-foundry/llmfoundry
  composer main.py yamls/mosaic_gpt/350m.yaml \
    train_loader.dataset.split=train_small \
    eval_loader.dataset.split=val_small \
    data_local=./pile-of-law \
    max_duration=100000ba \
    eval_interval=0 \
    data_remote=oci://mosaicml-internal-datasets/pile-of-law/small-1m-train/pretok-gpt-neox-20b/ \
    save_interval=1000ba \
    save_num_checkpoints_to_keep=1 \
    save_folder=oci://mosaicml-internal-checkpoints/sasha/{run_name}/checkpoints \
    loggers.wandb="{}" \
    tokenizer_name="EleutherAI/gpt-neox-20b"


image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04

# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME
run_name: mosaic-gpt-350m-gpus-8

gpu_num: 8
# gpu_type: a100_40gb
cluster: vision-cluster # replace with your cluster here!

# The below is injected as a YAML file: /mnt/config/parameters.yaml
# but is not used in this example.
parameters: {}

