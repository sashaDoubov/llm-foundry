max_seq_len: 256
seed: 1
model_name_or_path: mosaicml/mpt-7b-storywriter #mosaicml/mpt-7b # -storywriter #mosaicml/mpt-7b #EleutherAI/gpt-neo-125M
precision: amp_fp16 #fp32

# Tokenizer
tokenizer:
  name: ${model_name_or_path}
  kwargs:
    model_max_length: ${max_seq_len}

model:
  name: hf_causal_lm
  pretrained_model_name_or_path: ${model_name_or_path}
  init_device: cpu
  pretrained: true
  config_overrides:
    max_seq_len: ${max_seq_len}
    use_cache: False
    attn_config:
      attn_impl: triton

load_path: # Add your (optional) Composer checkpoint path here!

device_eval_batch_size: 1

# FSDP config for model sharding
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: FULL

icl_tasks:
-
  # label: triviaqa
  # dataset_uri: eval/local_data/triviaqa.jsonl
  # num_fewshot:
  # - 0
  # batch_size: 1
  # icl_task_type: question_answering
  # metric_names:
  # - InContextLearningQAF1
  # prompt_string: '' # this goes at the beginning of each input
  # example_delimiter: "\n" # this goes between fewshot examples
  # continuation_delimiter: ' ' # this separates questions from answers

  label: qspr
  dataset_uri: oci://mosaicml-internal-datasets/scrolls-icl/qasper_small.jsonl #/root/quality_medium.jsonl
  num_fewshot: [0]
  icl_task_type: question_answering
  metric_names:
  - InContextLearningQAF1
  prompt_string: 'The following passage has a question at the beginning, followed by the text. Return the answer to the question at the beginning of the text.\n' # this goes at the beginning of each input
  example_delimiter: "\n" # this goes between fewshot examples
  continuation_delimiter: 'Answer: ' # this separates questions from answers
# -
#   label: lambada_openai
#   dataset_uri: eval/local_data/lambada_openai.jsonl
#   num_fewshot: [0]
#   icl_task_type: language_modeling
