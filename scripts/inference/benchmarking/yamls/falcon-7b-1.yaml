benchmark_name: Falcon-alibi-7B_1xA100-80GB-bf16-GPT-7B

max_seq_len: 2048

model_name_or_path: tiiuae/falcon-7b
precision: bf16


  # Tokenizer
tokenizer:
  name: ${model_name_or_path}
  kwargs:
    model_max_length: ${max_seq_len}

model:
  name: hf_causal_lm
  pretrained_model_name_or_path: ${model_name_or_path}
  device: cpu
  pretrained: false
  vocab_size: 65024
  use_auth_token: false
  config_overrides:
    pad_token_id: null
    eos_token_id: null
    hidden_size: 4096
    n_head: 64
    # use_cache: false
    # multi_query: false
    # alibi: true


# # Tokenizer
# tokenizer:
#   name: ${model_name_or_path}
#   kwargs:
#     model_max_length: ${max_seq_len}
#     pad_token: <|endoftext|>

# model:
#   name: hf_causal_lm
#   vocab_size: 65024
#   pretrained_model_name_or_path: ${model_name_or_path}
#   init_device: cpu
#   pretrained: false

device: null
model_dtype: bf16
autocast_dtype: null
use_deepspeed: false

batch_sizes: [1] #[1, 2, 4, 8, 16, 32, 64]
input_lengths: [512]
# output_lengths: [64]
output_lengths: [64]
use_cache: true

num_batches: 5
num_warmup_batches: 3

# num_batches: 1
# num_warmup_batches: 0
